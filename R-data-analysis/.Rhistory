hardham.docs <- dir(hardham.path)
hardham.docs <- hardham.docs[which(hardham.docs != "cmds")]
all.hardham <- sapply(hardham.docs,
function(p) get.msg(file.path(hardham.path, p)))
# 邮件正文提取完毕
head(all.hardham)
hardham.tdm <- get.tdm(all.hardham) # 1. 词项-文档矩阵
hardham.tdm.mat <- as.matrix(hardham.tdm) # 2. 矩阵化
hardham.word.freq <- rowSums(hardham.tdm.mat)
hardham.df <- data.frame(cbind(names(hardham.word.freq), as.numeric(hardham.word.freq)), stringsAsFactors = FALSE)
head(hardham.df)
names(hardham.df) <- c("term", "frequency") # 3. 添加名称
hardham.df$frequency <- as.numeric(hardham.df$frequency)
hardham.occurrence <- sapply(1:nrow(hardham.tdm.mat),
function(i)
{
length(which(hardham.tdm.mat[i, ] > 0)) / ncol(hardham.tdm.mat)
})
head(hardham.occurrence)
# 3. hardham 文本向量
hardham.docs <- dir(hardham.path)
hardham.docs <- hardham.docs[which(hardham.docs != "cmds")]
all.hardham <- sapply(hardham.docs,
function(p) get.msg(file.path(hardham.path, p)))
# 邮件正文提取完毕
head(all.hardham)
# use classify function for testing
hardham.spamtest <- classify.email(all.hardham, train.df = spam.df)
hardham.easyhamtest <- classify.email(all.hardham, train.df = easyham.df)
hardham.res <- ifelse(hardham.spamtest > hardham.easyhamtest,
TRUE,
FALSE)
summary(hardham.res)
hardham.spamtest <- classify.email(all.hardham, train.df = spam.df)
R.version$arch
setwd("~/GitHub/eledata-data-analysis/R Data Mining/Data Mining Projects/Classification-NaiveBayes-SpamFilter")
# 垃圾邮件分类
require(tm)
# 设置数据路径
spam.path <- file.path("data", "spam")
spam2.path <- file.path("data", "spam_2")
easyham.path <- file.path("data", "easy_ham")
easyham2.path <- file.path("data", "easy_ham_2")
hardham.path <- file.path("data", "hard_ham")
hardham2.path <- file.path("data", "hard_ham_2")
# 读取邮件正文, path 表示文件路径和文件名称 /spam/text.txt, 返回文本向量
get.msg <- function(path){
open_file <- file(path, open = "rt", encoding = "latin1")
mail_text <- readLines(open_file) # 读完文件之后，返回向量化文本，mail_text[1] 表示第一行内容
mt_start <- as.numeric(which(mail_text == "")[1] + 1)
mt_len <- as.numeric(length(mail_text))
if(mt_start > mt_len) mt_start = mt_len - 1
mail_content <- mail_text[seq(mt_start, mt_len, 1)]
close(open_file)
return(paste(mail_content, collapse = "\n"))
}
# 构造词项-文档矩阵 TDM [i,j], 词项i在文档j中出现的次数
get.tdm <- function(doc.vec){
doc.corpus <- Corpus(VectorSource(doc.vec)) # 语料库
control <- list(stopwords = TRUE, removePunctuation = TRUE, removeNumbers = TRUE)
doc.tdm <- TermDocumentMatrix(doc.corpus, control) #计算词项-文档矩阵 TDM，
return(doc.tdm)
}
# 查询词项出现个数
count.word <- function(doc.vec, term){
tdm <- get.tdm(doc.vec)
word.freq <- rowSums(as.matrix(tdm))
term.freq <- word.freq[which(names(word.freq) == term)]
return (ifelse(length(term.freq) > 0, term.freq, 0))
}
# 检验垃圾邮件
classify.email <- function(doc.vec, train.df, prior = 0.5, c = 1e-6){
tdm <- get.tdm(doc.vec)
msg.freq <- rowSums(as.matrix(tdm))
msg.match <- intersect(names(msg.freq), train.df$term)
if(length(msg.match)){
return (prior*c^(length(msg.freq)))
}else{
# 利用贝叶斯定理来求解
match.probs <- train.df$occurrence[match(msg.match, train.df$term)]
return(prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
}
}
# 1. Spam 文本向量
spam.docs <- dir(spam.path)
spam.docs <- spam.docs[which(spam.docs != "cmds")]
all.spam <- sapply(spam.docs,
function(p) get.msg(file.path(spam.path, p)))
# 邮件正文提取完毕
head(all.spam)
# 信息整理
spam.tdm <- get.tdm(all.spam) # 1. 词项-文档矩阵
spam.tdm.mat <- as.matrix(spam.tdm) # 2. 矩阵化
word.freq <- rowSums(spam.tdm.mat)
spam.df <- data.frame(cbind(names(word.freq), as.numeric(word.freq)), stringsAsFactors = FALSE)
head(spam.df)
names(spam.df) <- c("term", "frequency") # 3. 添加名称
spam.df$frequency <- as.numeric(spam.df$frequency)
spam.occurrence <- sapply(1:nrow(spam.tdm.mat),
function(i)
{
length(which(spam.tdm.mat[i, ] > 0)) / ncol(spam.tdm.mat)
})
head(spam.occurrence)
spam.density <- spam.df$frequency/sum(spam.df$frequency)
spam.df <- transform(spam.df, density = spam.density, occurrence = spam.occurrence)
head(spam.df)
# 2. easyham 文本向量
easyham.docs <- dir(easyham.path)
easyham.docs <- easyham.docs[which(easyham.docs != "cmds")]
all.easyham <- sapply(easyham.docs,
function(p) get.msg(file.path(easyham.path, p)))
# 邮件正文提取完毕
head(all.easyham)
# 信息整理
easyham.tdm <- get.tdm(all.easyham) # 1. 词项-文档矩阵
easyham.tdm.mat <- as.matrix(easyham.tdm) # 2. 矩阵化
easyham.word.freq <- rowSums(easyham.tdm.mat)
easyham.df <- data.frame(cbind(names(easyham.word.freq), as.numeric(easyham.word.freq)), stringsAsFactors = FALSE)
head(easyham.df)
names(easyham.df) <- c("term", "frequency") # 3. 添加名称
easyham.df$frequency <- as.numeric(easyham.df$frequency)
easyham.occurrence <- sapply(1:nrow(easyham.tdm.mat),
function(i)
{
length(which(easyham.tdm.mat[i, ] > 0)) / ncol(easyham.tdm.mat)
})
head(easyham.occurrence)
easyham.density <- easyham.df$frequency/sum(easyham.df$frequency)
easyham.df <- transform(spam.df, density = easyham.density, occurrence = easyham.occurrence)
head(easyham.df)
# 3. hardham 文本向量
hardham.docs <- dir(hardham.path)
hardham.docs <- hardham.docs[which(hardham.docs != "cmds")]
all.hardham <- sapply(hardham.docs,
function(p) get.msg(file.path(hardham.path, p)))
# 邮件正文提取完毕
head(all.hardham)
hardham.spamtest <- classify.email(all.hardham, train.df = spam.df)
hardham.easyhamtest <- classify.email(all.hardham, train.df = easyham.df)
hardham.res <- ifelse(hardham.spamtest > hardham.easyhamtest,
TRUE,
FALSE)
summary(hardham.res)
hardham.spamtest
html.spam <- sapply(spam.docs,
function(p) count.word(all.spam, "html"))
head(spam.df[with(spam.df, order(-occurrence)),])
head(easyham.df[with(easyham.df, order(-occurrence)),])
head(all.hardham)
html.easyham <- count.word(all.easyham, "html")
html.easyham
table.easyham <- count.word(all.easyham, "table")
easyham.init <- cbind(html.easyham, table.easyham, "EASYHAM")
easyham.init
head(easyham.df[with(easyham.df, order(-occurrence)),])
init.df <- data.frame(rbind(spam.init, easyham.init),
stringsAsFactors = FALSE)
names(init.df) <- c("html", "table", "type")
init.df$html <- as.numeric(init.df$html)
init.df$table <- as.numeric(init.df$table)
init.df$type <- as.factor(init.df$type)
html.spam <- count.word(all.spam, "html")
table.spam <- count.word(all.spam, "table")
spam.init <- cbind(html.spam, table.spam, "SPAM")
init.df <- data.frame(rbind(spam.init, easyham.init),
stringsAsFactors = FALSE)
names(init.df) <- c("html", "table", "type")
init.df$html <- as.numeric(init.df$html)
init.df$table <- as.numeric(init.df$table)
init.df$type <- as.factor(init.df$type)
init.df
head(all.hardham)
doc.vec <- all.hardham
train.df <- spam.df
prior <- 0.5
c <- 1e-6
tdm <- get.tdm(doc.vec)
msg.freq <- rowSums(as.matrix(tdm))
msg.match <- intersect(names(msg.freq), train.df$term)
if(length(msg.match)){
return (prior*c^(length(msg.freq)))
}else{
# 利用贝叶斯定理来求解
match.probs <- train.df$occurrence[match(msg.match, train.df$term)]
return(prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
}
msg.match
if(length(msg.match)){
prop <- prior*c^(length(msg.freq))
}else{
# 利用贝叶斯定理来求解
match.probs <- train.df$occurrence[match(msg.match, train.df$term)]
prop <- prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
}
prop
length(msg.match)
classify.email <- function(doc.vec, train.df, prior = 0.5, c = 1e-6){
tdm <- get.tdm(doc.vec)
msg.freq <- rowSums(as.matrix(tdm))
msg.match <- intersect(names(msg.freq), train.df$term)
if(length(msg.match) > 0){
return (prior*c^(length(msg.freq)))
}else{
# 利用贝叶斯定理来求解
match.probs <- train.df$occurrence[match(msg.match, train.df$term)]
return(prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
}
}
hardham.spamtest <- classify.email(all.hardham, train.df = spam.df)
hardham.easyhamtest <- classify.email(all.hardham, train.df = easyham.df)
hardham.res <- ifelse(hardham.spamtest > hardham.easyhamtest,
TRUE,
FALSE)
summary(hardham.res)
hardham.spamtest
hardham.easyhamtest
if(length(msg.match) > 0){
prop <- prior*c^(length(msg.freq))
}else{
# 利用贝叶斯定理来求解
match.probs <- train.df$occurrence[match(msg.match, train.df$term)]
prop <- prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
}
prop
if(length(msg.match) > 0){
prop <- prior*c^(length(msg.freq))
}else{
# 利用贝叶斯定理来求解
match.probs <- train.df$occurrence[match(msg.match, train.df$term)]
prop <- prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match))
}
prop
c
length(msg.freq)
c^(length(msg.freq))
c <- 1e^-6
c ^ (length(msg.freq))
c <- 1e-6
c ^ (length(msg.freq))
c <- 0.000001
c ^ (length(msg.freq))
# 检验垃圾邮件
classify.email <- function(doc.vec, train.df, prior = 0.5, c = 1e-6){
tdm <- get.tdm(doc.vec)
msg.freq <- rowSums(as.matrix(tdm))
msg.match <- intersect(names(msg.freq), train.df$term)
if(length(msg.match) < 1){
return (prior*c^(length(msg.freq)))
}else{
# 利用贝叶斯定理来求解
match.probs <- train.df$occurrence[match(msg.match, train.df$term)]
return(prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
}
}
hardham.spamtest <- classify.email(all.hardham, train.df = spam.df)
hardham.easyhamtest <- classify.email(all.hardham, train.df = easyham.df)
hardham.res <- ifelse(hardham.spamtest > hardham.easyhamtest,
TRUE,
FALSE)
summary(hardham.res)
match.probs <- train.df$occurrence[match(msg.match, train.df$term)]
match.probs
prop <- prior * prod(match.probs)
prop
prod(match.probs)
classify.email <- function(doc.vec, train.df, prior = 0.5, c = 1e-6){
tdm <- get.tdm(doc.vec)
msg.freq <- rowSums(as.matrix(tdm))
msg.match <- intersect(names(msg.freq), train.df$term)
if(length(msg.match) < 1)
{
return(prior * c ^ (length(msg.freq)))
}
else
{
# 利用贝叶斯定理来求解
match.probs <- train.df$occurrence[match(msg.match, train.df$term)]
return(prior * prod(match.probs) * c ^ (length(msg.freq) - length(msg.match)))
}
}
hardham.spamtest <- classify.email(all.hardham, train.df = spam.df)
hardham.easyhamtest <- classify.email(all.hardham, train.df = easyham.df)
hardham.res <- ifelse(hardham.spamtest > hardham.easyhamtest,
TRUE,
FALSE)
summary(hardham.res)
print(prod(1:7)) == print(gamma(8))
print(prod(1:7))
gamma(8)
prod(1:7)
prod(1)
prod(1:10)
prod(1:2)
prod(1:3)
prod(1:4)
prod(0.1:0.9)
0.1:0.9
prod(c(0.1,0.2,0.3,0.4))
setwd("~/GitHub/ML_for_Hackers-master/05-Regression/data")
# 体重相对身高的散点图
require(ggplot2)
height.weight <- read.csv2("01_heights_weights_genders.csv", header = TRUE, sep = ",")
head(height.weight)
hw_point <- ggplot(height.weight, aes(Height, Weight)) +
geom_point(aes(colour = factor(Gender))) +
theme_bw() +
xlab("体重") +
ylab("身高")
hw_point
require(ggplot2)
height.weight <- read.csv2("01_heights_weights_genders.csv", header = TRUE, sep = ",")
head(height.weight)
hw_point <- ggplot(height.weight, aes(x = Height, y = Weight)) +
geom_point(aes(colour = factor(Gender))) +
theme_bw() +
xlab("体重") +
ylab("身高")
hw_point
hw_point <- ggplot(height.weight, aes(x = Height, y = Weight)) + geom_point()
hw_point <- ggplot(height.weight, aes(x = Height, y = Weight)) + geom_point()
hw_point
setwd("~/GitHub/eledata-data-analysis/R Stats and Model/data")
# 1000人寿命密度图，以是否吸烟为因子
require(ggplot2)
longevity <- read.csv2("longevity.csv",header = TRUE, sep = ",")
longevity$Smokes <- as.factor(longevity$Smokes)
smoke_life <- ggplot(longevity, aes(AgeAtDeath, fill = Smokes)) +
geom_density(position = "stack") +
facet_grid(Smokes ~ .) +
theme_bw() +
xlab("寿命密度") +
ylab("去世年龄")
smoke_life
require(ggplot2)
height.weight <- read.csv2("01_heights_weights_genders.csv", header = TRUE, sep = ",")
head(height.weight)
hw_point <- ggplot(height.weight, aes(Height, Weight)) +
geom_point(aes(colour = factor(Gender))) +
theme_bw() +
xlab("体重") +
ylab("身高")
hw_point
snow <- data.frame(
X = c(5.1,3.5,7.1,6.2,8.8,7.8,4.5,5.6,8,6.4),
Y = c(1907,1287,2700,2373,3260,3000,1947,2273,3113,2493)
)
plot(snow$X,snow$Y)
snow.lm <- lm(Y ~ 1 + X, data = snow)
summary(snow.lm)
Y <- predict(snow.lm, data.frame(X = 7),interval = "prediction",level = 0.95)
Y
# 线性回归
require(ggplot2)
# 1 积雪融化对下游灌溉的影响
snow <- data.frame(
X = c(5.1,3.5,7.1,6.2,8.8,7.8,4.5,5.6,8,6.4),
Y = c(1907,1287,2700,2373,3260,3000,1947,2273,3113,2493)
)
snow.plot <- ggplot(snow, aes(X, Y)) +
geom_point()
snow.plot
snow.plot <- ggplot(snow, aes(X, Y)) +
geom_point(colour)
snow.plot
# 线性回归
require(ggplot2)
# 1 积雪融化对下游灌溉的影响
snow <- data.frame(
X = c(5.1,3.5,7.1,6.2,8.8,7.8,4.5,5.6,8,6.4),
Y = c(1907,1287,2700,2373,3260,3000,1947,2273,3113,2493)
)
snow.plot <- ggplot(snow, aes(X, Y)) +
geom_point()
snow.plot
snow.lm <- lm(Y ~ 1 + X, data = snow)
summary(snow.lm)
# 线性回归
require(ggplot2)
# 1 积雪融化对下游灌溉的影响
snow <- data.frame(
X = c(5.1,3.5,7.1,6.2,8.8,7.8,4.5,5.6,8,6.4),
Y = c(1907,1287,2700,2373,3260,3000,1947,2273,3113,2493)
)
snow.lm <- lm(Y ~ 1 + X, data = snow)
summary(snow.lm)
snow.plot <- ggplot(snow, aes(X, Y)) +
geom_point() +
geom_smooth(lm = snow.lm)
snow.plot
snow.plot <- ggplot(snow, aes(X, Y)) +
geom_point() +
geom_smooth(method = lm)
snow.plot
# 线性回归
require(ggplot2)
# 1 积雪融化对下游灌溉的影响
snow <- data.frame(
X = c(5.1,3.5,7.1,6.2,8.8,7.8,4.5,5.6,8,6.4),
Y = c(1907,1287,2700,2373,3260,3000,1947,2273,3113,2493)
)
snow.lm <- lm(Y ~ 1 + X, data = snow)
summary(snow.lm)
snow.plot <- ggplot(snow, aes(X, Y)) +
geom_point() +
geom_smooth(method = snow.lm)
snow.plot
# 线性回归
require(ggplot2)
# 1 积雪融化对下游灌溉的影响
snow <- data.frame(
X = c(5.1,3.5,7.1,6.2,8.8,7.8,4.5,5.6,8,6.4),
Y = c(1907,1287,2700,2373,3260,3000,1947,2273,3113,2493)
)
snow.plot <- ggplot(snow, aes(X, Y)) +
geom_point() +
geom_smooth(method = lm)
snow.plot
snow.lm <- lm(Y ~ 1 + X, data = snow)
summary(snow.lm)
Y <- predict(snow.lm, data.frame(X = 7),interval = "prediction",level = 0.95)
Y
Anscombe<-data.frame(
X =c(10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0),
Y1=c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68),
Y2=c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74),
Y3=c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.44, 5.73),
X4=c(rep(8,7), 19, rep(8,3)),
Y4=c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89)
)
Anscombe.plot <- ggplot(Anscombe, aes(Y1, X)) +
geom_point() +
geom_smooth(method = lm)
Anscombe.plot
install.packages("knitr")
install.packages("corrplot")
(clp <- cor(longley, method = "pearson"))
(clS <- cor(longley, method = "spearman"))
(clS <- cor(longley, method = "spearman"))
corrplot(clp, method = "number")
corrplot(clS, method = "number")
corrplot(clS, method = "number")
require(corrplot)
(clp <- cor(longley, method = "pearson"))
(clS <- cor(longley, method = "spearman"))
(clS <- cor(longley, method = "spearman"))
corrplot(clp, method = "number")
corrplot(clS, method = "number")
corrplot(clS, method = "number")
## How much do they differ?
i <- lower.tri(Cl)
cor(cbind(P = clp[i], S = clS[i], K = clK[i]))
i <- lower.tri(clp)
cor(cbind(P = clp[i], S = clS[i], K = clK[i]))
(clp <- cor(longley, method = "pearson"))
(cls <- cor(longley, method = "spearman"))
(clk <- cor(longley, method = "kendall"))
corrplot(clp, method = "number")
corrplot(cls, method = "number")
corrplot(clk, method = "number")
## How much do they differ?
i <- lower.tri(clp)
cor(cbind(P = clp[i], S = cls[i], K = clk[i]))
i
C1 <- cov(swiss)
C1
swiss
# 协方差矩阵
require(corrplot)
x1 <- c(65,70,70,69,66,67,68,72,66,68)
x2 <- c(45,45,48,46,50,46,47,43,47,48)
x3 <- c(27.6 ,30.7,31.8,32.6,31,31.3,37,33.6,33.1,34.2)
test_data <- cbind(x1, x2, x3)
test_data
# var 方差函数，也可以计算出协方差矩阵。
var(test_data)
cov(test_data)
# cor 相关新系数
cor_t <- cor(test_data)
symnum(cor_t)
corrplot(cor_t, method = "circle")
corrplot(clp, method = "circle")
corrplot(clk, method = "pie")
install.packages(c("descr", "formatR", "GGally", "ggdendro", "highr", "Hmisc", "Matrix", "multcomp", "nlme", "openssl", "quantreg", "Rcpp", "rmarkdown", "robustbase", "stringi", "survival", "zoo"))
library("rattle")
rattle()
rattle()
crs$dataset <- read.csv("file:///C:/Program Files/R/R-3.2.5/library/rattle/csv/audit.csv", na.strings=c(".", "NA", "", "?"), strip.white=TRUE, encoding="UTF-8")
crs$dataset
head(crs$dataset)
x<-1:50
set.seed(10)
y<-sample(x,10)
x
y
y
y
y
x<-1:50
y<-sample(x,10)
y
y
y
y
x<-1:50
y<-sample(x,10)
y
x<-1:50
set.seed(10)
y<-sample(x,10)
y
y
