{
    "contents" : "data(iris)\nhead(iris)#提取前 6 行的数据\nlibrary(fpc)#安装程序包 fpc 并调用\n#将数据标准化\nnorm <- function(x){\n    (x-mean(x))/(sqrt(var(x)))\n}\nraw.data <- iris[,1:4]\nhead(raw.data)\nnorm.data <- data.frame(sl = norm(raw.data[,1]),\n                        sw = (raw.data[,2]),\n                        pl = (raw.data[,3]),\n                        pw = (raw.data[,4]))\nhead(norm.data)\n# k 取 2 到 10，评估 K\nK <- 2:10\nround <- 40 # 每次迭代 40 次，避免局部最优\nrst <- sapply(K, function(i){\n    print(paste(\"K=\",i))\n    mean(sapply(1:round,function(r){\n        print(paste(\"Round\",r))\n        result <- kmeans(norm.data, i)\n        stats <- cluster.stats(dist(norm.data), result$cluster)\n        stats$avg.silwidth }))\n})\nplot(K,rst,type='l',main='轮廓系数与 K 的关系', ylab='轮廓系数')\n#当 k 取 2 时，有最大的轮廓系数，所以选取聚类种类为 2 #\n#将聚类结果通过多维定标(Multidimensional scaling)降至 2 维，进而查看聚类结果#\nold.par <- par(mfrow = c(1,2))\nk = 2 # 根据上面的评估 k=2 最优\nclu <- kmeans(norm.data,k)\nmds = cmdscale(dist(norm.data,method=\"euclidean\"))\nplot(mds, col=clu$cluster, main='kmeans 聚类 k=2', pch = 19)\nplot(mds, col=iris$Species, main='原始聚类', pch = 19)\npar(old.par)\n##由结果可知，由于原始数据集分\n\nhclust()\n\nagnes()\n\n\ndaisy()\n\n\ndata(ruspini)\npr4 <- pam(ruspini, 4)\nstr(si <- silhouette(pr4))\n(ssi <- summary(si))\nplot(si) # silhouette plot\nplot(si, col = c(\"red\", \"green\", \"blue\", \"purple\"))# with cluster-wise coloring\n\nsi2 <- silhouette(pr4$clustering, dist(ruspini, \"canberra\"))\nsummary(si2) # has small values: \"canberra\"'s fault\nplot(si2, nmax= 80, cex.names=0.6)\n\nop <- par(mfrow= c(3,2), oma= c(0,0, 3, 0),\n          mgp= c(1.6,.8,0), mar= .1+c(4,2,2,2))\nfor(k in 2:6)\n  plot(silhouette(pam(ruspini, k=k)), main = paste(\"k = \",k), do.n.k=FALSE)\nmtext(\"PAM(Ruspini) as in Kaufman & Rousseeuw, p.101\",\n      outer = TRUE, font = par(\"font.main\"), cex = par(\"cex.main\")); frame()\n\n## the same with cluster-wise colours:\nc6 <- c(\"tomato\", \"forest green\", \"dark blue\", \"purple2\", \"goldenrod4\", \"gray20\")\nfor(k in 2:6)\n  plot(silhouette(pam(ruspini, k=k)), main = paste(\"k = \",k), do.n.k=FALSE,\n       col = c6[1:k])\npar(op)\n\n## clara(): standard silhouette is just for the best random subset\ndata(xclara)\nset.seed(7)\nstr(xc1k <- xclara[sample(nrow(xclara), size = 1000) ,])\ncl3 <- clara(xc1k, 3)\nplot(silhouette(cl3))# only of the \"best\" subset of 46\n## The full silhouette: internally needs large (36 MB) dist object:\nsf <- silhouette(cl3, full = TRUE) ## this is the same as\ns.full <- silhouette(cl3$clustering, daisy(xc1k))\nif(paste(R.version$major, R.version$minor, sep=\".\") >= \"2.3.0\")\n  stopifnot(all.equal(sf, s.full, check.attributes = FALSE, tolerance = 0))\n## color dependent on original \"3 groups of each 1000\":\nplot(sf, col = 2+ as.integer(names(cl3$clustering) ) %/% 1000,\n     main =\"plot(silhouette(clara(.), full = TRUE))\")\n\n## Silhouette for a hierarchical clustering:\nar <- agnes(ruspini)\nsi3 <- silhouette(cutree(ar, k = 5), # k = 4 gave the same as pam() above\n                  daisy(ruspini))\nplot(si3, nmax = 80, cex.names = 0.5)\n## 2 groups: Agnes() wasn't too good:\nsi4 <- silhouette(cutree(ar, k = 2), daisy(ruspini))\nplot(si4, nmax = 80, cex.names = 0.5)\n\n",
    "created" : 1465971374758.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4001894678",
    "id" : "FBEDF263",
    "lastKnownWriteTime" : 1465897614,
    "path" : "~/GitHub/eledata-data-analysis/R-data-analysis/R Data Mining/Data Mining Algorithm/Cluster/Kmeans.R",
    "project_path" : "R Data Mining/Data Mining Algorithm/Cluster/Kmeans.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}